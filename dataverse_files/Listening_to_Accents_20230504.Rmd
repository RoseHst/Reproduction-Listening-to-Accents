---
title: "Data analysis: Listening to Accents: Comprehensibility, accentedness and intelligibility of regional and non-native varieties of English"
author: "Gil Verbeke"
date: '2023-05-04'
output: pdf_document
---

# Data

Verbeke, Gil; Simon, Ellen, 2023, "Replication Data for: Listening to Accents: Comprehensibility, accentedness and intelligibility of native and non-native English speech", https://doi.org/10.18710/8F0Q0L, DataverseNO.

# Preliminary steps 

```{r}
# Load required packages. If these packages have not been installed yet, use install.packages("package name") before loading.

library("ggplot2")
library("GGally")
library("lme4")
library("Matrix")
library("dplyr")
library("lattice") 
library("tidyverse") 
library("effects")
library("ggpubr")
library("afex")
library("car")
library("ggeffects")
library("sjmisc")
```


----------------------
# STIMULUS SELECTION #
----------------------

# Load data

```{r}
Stimuli_T2.data <- read.csv("Listening_to_Accents_Stimuli_Task_2.csv", row.names = NULL)
```

# Inspection of the dataframe

```{r}
str(Stimuli_T2.data)
summary(Stimuli_T2.data)
```

# Convert character variables to factors

```{r}
Stimuli_T2.data$Accent <- as.factor(Stimuli_T2.data$Accent)
```

# Testing for Inter-stimulus similiarity

We want to control the selected speech samples for similarity along several dimensions, such as the duration of the excerpt, speaking rate, lexical frequency of the content words etc. To that end, we perform multiple one-way ANOVA tests 

## One-way ANOVAs

```{r}
Duration.aov <- aov(Duration ~ Accent, data = Stimuli_T2.data)
Syllables.aov <- aov(Syllables ~ Accent, data = Stimuli_T2.data)
Rate.aov <- aov(Rate ~ Accent, data = Stimuli_T2.data)
Frequency.aov <- aov(Frequency ~ Accent, data = Stimuli_T2.data)

summary(Duration.aov)
summary(Syllables.aov)
summary(Rate.aov)
summary(Frequency.aov)
```

There is a significant difference between the Varieties in terms of the number of syllables per sentence. Post-hoc tests will be performed to reveal which Varieties differ significantly.

```{r}
TukeyHSD(Syllables.aov)
```

There is a significant difference between Indian English and Chinese accented English in terms of the number of syllables per utterance.

## Assumptions

```{r}
# Test for homogeneity of variance 

leveneTest(Duration ~ Accent, data = Stimuli_T2.data)
leveneTest(Syllables ~ Accent, data = Stimuli_T2.data)
leveneTest(Rate ~ Accent, data = Stimuli_T2.data)
leveneTest(Frequency ~ Accent, data = Stimuli_T2.data)
```

The output shows that the p-value is never less than 0.05 (i.e. the significance level). This indicates that there is no evidence that the variance across the different groups is significantly different.


```{r}
# Test for normality

Duration_residuals <- residuals(object = Duration.aov)
Syllables_residuals <- residuals(object = Syllables.aov)
Rate_residuals <- residuals(object = Rate.aov)
Frequency_residuals <- residuals(object = Frequency.aov)

shapiro.test(x = Duration_residuals )
shapiro.test(x = Syllables_residuals )
shapiro.test(x = Rate_residuals )
shapiro.test(x = Frequency_residuals )
```

The Shapiro-Wilk test on the ANOVA residuals for both Duration and Syllables indicates that normality is violated. A non-parametric alternative to one-way ANOVA test is Kruskal-Wallis rank sum test.

```{r}
kruskal.test(Duration ~ Accent, data = Stimuli_T2.data)
kruskal.test(Syllables ~ Accent, data = Stimuli_T2.data)
```


------------------------------------
# COMPREHENSIBILITY & ACCENTEDNESS #
------------------------------------

# Load data

```{r}
ComprAcc.data <- read.csv("Listening_to_Accents_Comprehensibility_Accentedness.csv", row.names = NULL)
```
# Inspection of the dataframe

```{r}
str(ComprAcc.data)
summary(ComprAcc.data)
```

# Convert character variables to factors

```{r}
ComprAcc.data$Participant <- as.factor(ComprAcc.data$Participant)
ComprAcc.data$Familiarity <- as.factor(ComprAcc.data$Familiarity)
ComprAcc.data$Accent <- as.factor(ComprAcc.data$Accent)
summary(ComprAcc.data)
```

#Relevel Varieties so as to order them according to Kachru's (1985) model (Inner Circle, Outer Circle, Expanding Circle)

```{r}
ComprAcc.data$Accent <- factor(ComprAcc.data$Accent, levels = c("GBE", "GAE", "NBE", "SAE", "IndEng", "NigEng", "ChinEng", "SpanEng"))
levels(ComprAcc.data$Accent)
```

# Rename the levels of Accent

```{r}
levels(ComprAcc.data$Accent) <- c("General British English", "General American English", "Newcastle English", "Texan English", "Indian English", "Nigerian English", "Chinese English", "Spanish English")
summary(ComprAcc.data)
```


# Rename the levels of Familiarity to get a categorical variable with two levels

```{r}
levels(ComprAcc.data$Familiarity) <- c("Unfamiliar", "Familiar", "Unfamiliar", "Familiar", "Familiar")
summary(ComprAcc.data)
```

# Figure 1: Comprehensibility and Accentedness ratings per Accent

```{r, fig.height=6,fig.width=7.5}
require(ggplot2)
p <- ggplot(ComprAcc.data %>% pivot_longer(cols = c('Comprehensibility', 'Accentedness'), 
                                                names_to = 'Test_Type',
                                                values_to = 'Score'),
            aes(x=Accent, y=Score)) + 
  geom_violin(aes(fill = Test_Type), size = 0.25) + 
  geom_boxplot(aes(fill = Test_Type), width = 0.05, position = position_dodge(0.9), size = 0.25) + 
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + 
  stat_summary(fun = mean, aes(y=Score, x = Accent, group=Test_Type), position = position_dodge(0.9), geom = "point", shape=23, size = 2, bg = "white") + 
  facet_wrap(~ Accent, scales="free") + 
  xlab("") + ylab("Scores") + ggtitle("") + 
  guides(fill=guide_legend(title="")) + 
  scale_fill_manual(values=c("#EA5051", "#689DC9")) + 
  scale_y_continuous(breaks = c(1, 3, 5, 7, 9), limits = c(1,9), labels = c("1", "3", "5", "7", "9")) + 
  theme(legend.position = c(0.895, 0.1))
p
```

# Regression model

```{r}
m1.ComprAcc <- lmer(Comprehensibility ~ Accentedness * Accent + (1|Participant) + (1 + Accentedness||Participant), data = ComprAcc.data)
m2.ComprAcc <- lmer(Comprehensibility ~ Accentedness + Accent + (1|Participant) + (1 + Accentedness||Participant), data = ComprAcc.data)
anova(m1.ComprAcc, m2.ComprAcc)
```


```{r}
m3.ComprAcc <- lmer(Comprehensibility ~ Accentedness * Accent + Familiarity + (1|Participant) + (1 + Accentedness||Participant), data = ComprAcc.data)
anova(m1.ComprAcc, m3.ComprAcc)
```

```{r}
summary(m1.ComprAcc)
```

```{r}
confint(m1.ComprAcc, method = "Wald")
```


# Residuals and visual inspection of the residuals

## Residual vs. fitted plot 

```{r}
m1.ComprAcc.res <- resid(m1.ComprAcc)
plot(fitted(m1.ComprAcc), m1.ComprAcc.res)
abline(0,0)
```

## QQ-plot

```{r}
qqnorm(m1.ComprAcc.res)
qqline(m1.ComprAcc.res)
```
Residuals slightly stray from the line near the right tail, which could indicate that the data are not normally distributed.

## Density plot and Histogram

```{r}
plot(density(m1.ComprAcc.res))
```

Density plot roughly follows a normal distribution.

```{r}
hist(m1.ComprAcc.res)
```


# Figure 2: Effect plot of the interaction 

```{r}
p1 <- ggpredict(m1.ComprAcc, terms = c("Accentedness", "Accent"))
ggplot(p1, aes(x, y = predicted, colour = group)) + 
  geom_line() +
  geom_point(size = 0.75) +
  scale_y_continuous(breaks = c(1,2,3,4,5), limits = c(0.5,5.5), labels = c("1", "2", "3", "4", "5")) +
  scale_x_continuous(breaks = c(1,3,5,7,9), limits = c(0.5,9.5), labels = c("1", "3", "5", "7", "9")) +
  labs(x = "Accentedness", y = "Comprehensibility", colour = "Accent") +
  theme(legend.title = element_text(size = 12, face = "bold"),
        legend.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10))
```


-------------------
# INTELLIGIBILITY #
-------------------

# Load data

```{r}
Intell.data <- read.csv("Listening_to_Accents_Intelligibility", row.names = NULL)
```

# Inspection of the dataframe

```{r}
str(Intell.data)
summary(Intell.data)
```

# Convert character variables to factors

```{r}
Intell.data$Participant <- as.factor(Intell.data$Participant)
Intell.data$Accent <- as.factor(Intell.data$Accent)
Intell.data$Sentence <- as.factor(Intell.data$Sentence)
Intell.data$Item <- as.factor(Intell.data$Item)
Intell.data$Word_Type <- as.factor(Intell.data$Word_Type)
Intell.data$Accuracy <- as.factor(Intell.data$Accuracy)
Intell.data$Familiarity <- as.factor(Intell.data$Familiarity)
summary(Intell.data)
```

# Relevel Varieties so as to order them according to Kachru's (1985) model (Inner Circle, Outer Circle, Expanding Circle)

```{r}
Intell.data$Accent <- factor(Intell.data$Accent, levels = c("GBE", "GAE", "NBE", "SAE", "IndEng", "NigEng", "ChinEng", "SpanEng"))
levels(Intell.data$Accent)
```

# Rename the levels of Accent

```{r}
levels(Intell.data$Accent) <- c("General British English", "General American English", "Newcastle English", "Texan English", "Indian English", "Nigerian English", "Chinese English", "Spanish English")
summary(Intell.data)
```

# Rename the levels of Familiarity to get a categorical variable with two levels

```{r}
levels(Intell.data$Familiarity)
```

```{r}
levels(Intell.data$Familiarity) <- c("Unfamiliar", "Familiar", "Unfamiliar", "Familiar", "Familiar")
summary(Intell.data)
```

# Rename the levels of Accuracy

```{r}
levels(Intell.data$Accuracy) <- c("Incorrect", "Correct")
```

# Regression model

## Checking for random intercepts and slopes

```{r}
m1.Intell <- glmer(Accuracy ~ Accent + (1|Participant) + (1 + Accent|Participant), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = Intell.data, family=binomial)
m2.Intell <- glmer(Accuracy ~ Accent + (1|Participant) + (1 + Accent|Participant) + (1|Sentence), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = Intell.data, family=binomial)
anova(m1.Intell, m2.Intell)
```

```{r}
m3.Intell <- glmer(Accuracy ~ Accent + (1|Participant) + (1 + Accent|Participant) + (1|Sentence/Item), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = Intell.data, family=binomial)
anova(m2.Intell, m3.Intell)
```

```{r}
summary(m3.Intell)
```


## Add predictor variables to the model


```{r}
m4.Intell <- glmer(Accuracy ~ Accent + Word_Type + (1|Participant) + (1 + Accent|Participant) + (1|Sentence/Item), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = Intell.data, family=binomial)
anova(m3.Intell, m4.Intell)
```

A model with Accent and Word Type as fixed factors (without interaction) seems to be a better fit to estimate transcription Accuracy.


```{r}
summary(m4.Intell)
```

```{r}
confint(m4.Intell, method = "Wald")
```


```{r}
m5.Intell <- glmer(Accuracy ~ Accent + Word_Type + Familiarity + (1|Participant) + (1 + Accent|Participant) + (1|Sentence/Item), control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = Intell.data, family=binomial)
anova(m4.Intell, m5.Intell)
```

# Figure 3: Effect plots of the predictor variables and random effects


```{r, fig.height=6,fig.width=7.5}
plot(allEffects(m5.Intell)[c(1,2)], ylim=c(0.5,1), main= "", rescale.axis=FALSE, axes = list(grid=FALSE, y=list(lab="Prob(Accuracy = Correct)"), x=list(rotate=57)), lines=list(col=c('#000000', '#000000')))
```

```{r, echo=FALSE}
dotplot(ranef(m5.Intell, which = "Participant", condVar = TRUE))
```

```{r, echo=FALSE}
dotplot(ranef(m5.Intell, which = "Sentence", condVar = TRUE))
```

```{r, echo=FALSE}
dotplot(ranef(m5.Intell, which = "Item", condVar = TRUE))
```


---------------------------------------
# COMPREHENSIBILITY & INTELLIGIBILITY #
---------------------------------------

```{r}
ComprIntell.data <- read.csv("Listening_to_Accents_Comprehensibility_Intelligibility", row.names = NULL)
```

```{r}
install.packages("performance")
library(performance)
```

# Inspection of the dataframe

```{r}
str(ComprIntell.data)
summary(ComprIntell.data)
```

```{r}
ComprIntell.data$Participant <- as.factor(ComprIntell.data$Participant)
ComprIntell.data$Accent <- as.factor(ComprIntell.data$Accent)
summary(ComprIntell.data)
```

#Relevel Varieties so as to order them according to Kachru's (1985) model (Inner Circle, Outer Circle, Expanding Circle)

```{r}
ComprIntell.data$Accent <- factor(ComprIntell.data$Accent, levels = c("GBE", "GAE", "NBE", "SAE", "IndEng", "NigEng", "ChinEng", "SpanEng"))
levels(ComprIntell.data$Accent)
```

# Rename the levels of Accent

```{r}
levels(ComprIntell.data$Accent) <- c("General British English", "General American English", "Newcastle English", "Texan English", "Indian English", "Nigerian English", "Chinese English", "Spanish English")
summary(ComprIntell.data)
```

# Figure 5: Scatterplots of correlation between Comprehensibility ratings and Intelligibility scores

```{r}
ggplot(ComprIntell.data, aes(x = Comprehensibility, y = Intelligibility)) + 
  geom_point(size = 0.5) +
  geom_smooth(method = "auto") +
  geom_jitter() +
  facet_wrap(~Accent)
```

# Regression model

## Checking for random intercepts and slopes

```{r}
m1.ComprIntell <- lmer(Intelligibility ~ Comprehensibility + Accent + (1|Participant) , data = ComprIntell.data)
m2.ComprIntell <- lmer(Intelligibility ~ Comprehensibility * Accent + (1|Participant) , data = ComprIntell.data)
anova(m1.ComprIntell, m2.ComprIntell)
```


```{r}
summary(m2.ComprIntell)
```



```{r}
m3.ComprIntell <- lmer(Intelligibility ~ Comprehensibility * Accent + (1|Participant) + (1 + Comprehensibility||Participant), data = ComprIntell.data)
anova(m2.ComprIntell, m3.ComprIntell)
```


```{r}
summary(m3.ComprIntell)
```


```{r}
confint(m3.ComprIntell, method = "Wald")
```


# Residuals and visual inspection of the residuals

## Residual vs. fitted plot 

```{r}
m2.ComprIntell.res <- resid(m2.ComprIntell)
plot(fitted(m2.ComprIntell), m2.ComprIntell.res)
abline(0,0)
```

## QQ-plot

```{r}
qqnorm(m2.ComprIntell.res)
qqline(m2.ComprIntell.res)
```
Residuals stray from the line near the left tail, which could indicate that the data are not normally distributed.

## Density plot and Histogram

```{r}
plot(density(m2.ComprIntell.res))
```

Density plot roughly follows a normal distribution.

```{r}
hist(m2.ComprIntell.res)
```


